{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dynamic module does not define module export function (PyInit_libPyROOT)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd75fabd5019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mROOT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgROOT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/build/lib/ROOT.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m### system and interpreter setup ------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcppyy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/build/lib/cppyy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m0x100\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;36m0x2\u001b[0m \u001b[0;34m)\u001b[0m    \u001b[0;31m# RTLD_GLOBAL | RTLD_NOW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mlibPyROOT\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m    \u001b[0;31m# reset dl flags if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dynamic module does not define module export function (PyInit_libPyROOT)"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ROOT import gROOT\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the IMDB data from keras for this lab. The data is already enocded so I wanted to show an example of what text data looks like before it gets encoded. Below is the stanford sentiment treebank data broken up into its data and the sentiment values.\n",
    "\n",
    "I wanted to use a more complex dataset for this but the time constraints due to COV-19 have made that difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>phrase_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! '</td>\n",
       "      <td>22935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! ''</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>179257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>22936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>220441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>179256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>élan</td>\n",
       "      <td>220442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>É</td>\n",
       "      <td>220443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>220444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase phrase_ids\n",
       "0                                                       !          0\n",
       "1                                                     ! '      22935\n",
       "2                                                    ! ''      18235\n",
       "3                                                  ! Alas     179257\n",
       "4                                             ! Brilliant      22936\n",
       "...                                                   ...        ...\n",
       "239227  zoning ordinances to protect your community fr...     220441\n",
       "239228                                          zzzzzzzzz     179256\n",
       "239229                                               élan     220442\n",
       "239230                                                  É     220443\n",
       "239231                   É um passatempo descompromissado     220444\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I import it as a table using pandas and I have to set the column names manually.\n",
    "data = pd.read_table('dictionary.txt',index_col=False, header=None)\n",
    "data.columns = ['Phrase|Index']\n",
    "# Most corpus text data is set up in a way that has delimiters to seperate the data from the indexs. The delimiter for this data is |\n",
    "data = data['Phrase|Index'].str.split('|', expand=True)\n",
    "# The code above split the strings into an id index and the actual data.\n",
    "data = data.rename(columns={0: 'Phrase', 1: 'phrase_ids'})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_ids</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239227</th>\n",
       "      <td>239227</td>\n",
       "      <td>0.36111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239228</th>\n",
       "      <td>239228</td>\n",
       "      <td>0.38889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239229</th>\n",
       "      <td>239229</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239230</th>\n",
       "      <td>239230</td>\n",
       "      <td>0.88889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239231</th>\n",
       "      <td>239231</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       phrase_ids sentiment_values\n",
       "0               0              0.5\n",
       "1               1              0.5\n",
       "2               2          0.44444\n",
       "3               3              0.5\n",
       "4               4          0.42708\n",
       "...           ...              ...\n",
       "239227     239227          0.36111\n",
       "239228     239228          0.38889\n",
       "239229     239229          0.33333\n",
       "239230     239230          0.88889\n",
       "239231     239231              0.5\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I then do the same steps for the sentiment value labels for this data.\n",
    "labels = pd.read_table('sentiment_labels.txt')\n",
    "labels = labels['phrase ids|sentiment values'].str.split('|', expand=True)\n",
    "labels = labels.rename(columns={0: 'phrase_ids', 1: 'sentiment_values'})\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>sentiment_values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phrase_ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22935</th>\n",
       "      <td>! '</td>\n",
       "      <td>0.52778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18235</th>\n",
       "      <td>! ''</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179257</th>\n",
       "      <td>! Alas</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22936</th>\n",
       "      <td>! Brilliant</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220441</th>\n",
       "      <td>zoning ordinances to protect your community fr...</td>\n",
       "      <td>0.13889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179256</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "      <td>0.19444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220442</th>\n",
       "      <td>élan</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220443</th>\n",
       "      <td>É</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220444</th>\n",
       "      <td>É um passatempo descompromissado</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase sentiment_values\n",
       "phrase_ids                                                                    \n",
       "0                                                           !              0.5\n",
       "22935                                                     ! '          0.52778\n",
       "18235                                                    ! ''              0.5\n",
       "179257                                                 ! Alas          0.44444\n",
       "22936                                             ! Brilliant          0.86111\n",
       "...                                                       ...              ...\n",
       "220441      zoning ordinances to protect your community fr...          0.13889\n",
       "179256                                              zzzzzzzzz          0.19444\n",
       "220442                                                   élan          0.51389\n",
       "220443                                                      É              0.5\n",
       "220444                       É um passatempo descompromissado              0.5\n",
       "\n",
       "[239232 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I merged the data into one table using a SQL like join command which combines the tables based on the 'phrase_ids' column.\n",
    "df = data.merge(labels, how='inner', on='phrase_ids')\n",
    "# I then set the phrase ids as the index for the dataframe and drop the redundant id column.\n",
    "df.index = df.phrase_ids\n",
    "df = df.drop('phrase_ids', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process is very common and you should practice it yourself to get used to the process. I recommend (this isn't graded) to do this yourself with the amazon review data here: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a pretrained embedder called GloVe. here I had to use the 'quoting=csv.QUOTE_NONE' because the text file included quotation marks.\n",
    "embed = pd.read_table('glove.6B.100d.txt', engine='python', encoding='utf-8', error_bad_lines=False, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the -0.038194 -0.24487 0.72812 -0.39961 0.0831...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, -0.10767 0.11053 0.59812 -0.54361 0.67396 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>. -0.33979 0.20941 0.46348 -0.64792 -0.38377 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of -0.1529 -0.24279 0.89837 0.16996 0.53516 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to -0.1897 0.050024 0.19084 -0.049184 -0.08973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>chanty -0.15577 -0.049188 -0.064377 0.2236 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>kronik -0.094426 0.14725 -0.15739 0.071966 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>rolonda 0.36088 -0.16919 -0.32704 0.098332 -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>sandberger 0.28365 -0.6263 -0.44351 0.2177 -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0       the -0.038194 -0.24487 0.72812 -0.39961 0.0831...\n",
       "1       , -0.10767 0.11053 0.59812 -0.54361 0.67396 0....\n",
       "2       . -0.33979 0.20941 0.46348 -0.64792 -0.38377 0...\n",
       "3       of -0.1529 -0.24279 0.89837 0.16996 0.53516 0....\n",
       "4       to -0.1897 0.050024 0.19084 -0.049184 -0.08973...\n",
       "...                                                   ...\n",
       "399995  chanty -0.15577 -0.049188 -0.064377 0.2236 -0....\n",
       "399996  kronik -0.094426 0.14725 -0.15739 0.071966 -0....\n",
       "399997  rolonda 0.36088 -0.16919 -0.32704 0.098332 -0....\n",
       "399998  zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.3...\n",
       "399999  sandberger 0.28365 -0.6263 -0.44351 0.2177 -0....\n",
       "\n",
       "[400000 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see here that the data has two components to it. The beginning of every string is the word that got embedded and it is followed by a long list of numbers.\n",
    "# These numbers are how the machine learning model trains on data. Normally you will automatically create a trainable embedding in your model (like we will today).\n",
    "# There are times when you will want to use a pretrained embedding like GloVe to speed up computation time. So its good to know how to use them.\n",
    "# You can find detailed instructions on how to use GloVe and other pre-trained embeddings in the keras and tensorflow docs.\n",
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets get into the data we are working with today. In the last couple of labs we used CNNs and ResNets a lot. This time we are going to compare CNNs with LSTMs for the purpose of classifying text. The data is setup so that a '0 label' is a negative review and a '1 label' is a postive review.\n",
    "\n",
    "We want to create machine learning models to automatically detect whether or not a review is positive. This has wide applications for both industry and research and has been extensively researched since 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: x=(25000, 400), y=(25000,)\n",
      "Test: x=(25000, 400), y=(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Lets load our data. We will limit the number of words to 5,000 as that is how the data is setup.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# We pad the data because not all sentences in our data are the same length. We want to use a number that is larger than our largest data. Here I will choose 400.\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=400)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=400)\n",
    "\n",
    "print('Train: x=%s, y=%s' % (x_train.shape, y_train.shape))\n",
    "print('Test: x=%s, y=%s' % (x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 74s 3ms/sample - loss: 0.6950 - accuracy: 0.6258 - val_loss: 0.6824 - val_accuracy: 0.5037\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3918 - accuracy: 0.8230 - val_loss: 0.5191 - val_accuracy: 0.8340\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.2914 - accuracy: 0.8788 - val_loss: 0.3857 - val_accuracy: 0.8243\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 66s 3ms/sample - loss: 0.2259 - accuracy: 0.9082 - val_loss: 0.2981 - val_accuracy: 0.8724\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.1798 - accuracy: 0.9288 - val_loss: 0.3092 - val_accuracy: 0.8741\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.1439 - accuracy: 0.9432 - val_loss: 0.3533 - val_accuracy: 0.8734\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.1193 - accuracy: 0.9543 - val_loss: 0.3423 - val_accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.0949 - accuracy: 0.9644 - val_loss: 0.3709 - val_accuracy: 0.8735\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.0836 - accuracy: 0.9676 - val_loss: 0.4421 - val_accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.0738 - accuracy: 0.9726 - val_loss: 0.3966 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8e8a1d5c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets start with a very simple 1D CNN model. We will use this as our baseline for everything else in this lab.\n",
    "model = Sequential()\n",
    "\n",
    "# This embedding is a trainable parameter. We aren't using GloVE for this model.\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# There isn't much of a difference with how 1D and 2D CNNs work. They still use filters and scan the data.\n",
    "# we will use a similar model as our 2D CNN with the adition of an embedding layer at the beginning.\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We will use a sigmoid and a 1 neuron dense output since our data is binary.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well is this model doing? Is it overfitting? If so how could you fix this since we are already applying BatchNorm and dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 32s 1ms/sample - loss: 0.7090 - accuracy: 0.5301 - val_loss: 0.6469 - val_accuracy: 0.6761\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.5566 - accuracy: 0.7194 - val_loss: 0.4559 - val_accuracy: 0.7893\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.4680 - accuracy: 0.7856 - val_loss: 0.4394 - val_accuracy: 0.7947\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.4263 - accuracy: 0.8065 - val_loss: 0.3959 - val_accuracy: 0.8212\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.4036 - accuracy: 0.8180 - val_loss: 0.4050 - val_accuracy: 0.8179\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 30s 1ms/sample - loss: 0.3814 - accuracy: 0.8308 - val_loss: 0.3386 - val_accuracy: 0.8623\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.3652 - accuracy: 0.8443 - val_loss: 0.3477 - val_accuracy: 0.8519\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.3543 - accuracy: 0.8440 - val_loss: 0.3351 - val_accuracy: 0.8588\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.3465 - accuracy: 0.8505 - val_loss: 0.3105 - val_accuracy: 0.8736\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.3362 - accuracy: 0.8572 - val_loss: 0.3077 - val_accuracy: 0.8748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70bae0f690>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try and apply those fixes here. Can you make a baseline that doesn't overfit? What worked best\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# This embedding is a trainable parameter. We aren't using GloVE for this model.\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Conv1D(16,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(16,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 399s 16ms/sample - loss: 0.7905 - accuracy: 0.6594 - val_loss: 0.6765 - val_accuracy: 0.5016\n",
      "Epoch 2/10\n",
      " 9600/25000 [==========>...................] - ETA: 3:46 - loss: 0.3816 - accuracy: 0.8326"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bc374537f499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now we will make the network more complex by adding more filters to the data. How did this affect training?\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# This embedding is a trainable parameter. We aren't using GloVE for this model.\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# There isn't much of a difference with how 1D and 2D CNNs work. They still use filters and scan the data.\n",
    "# we will use a similar model as our 2D CNN with the adition of an embedding layer at the beginning.\n",
    "model.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We will use a sigmoid and a 1 neuron dense output since our data is binary.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 130s 5ms/sample - loss: 0.6532 - accuracy: 0.6542 - val_loss: 0.6820 - val_accuracy: 0.5231\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 145s 6ms/sample - loss: 0.3685 - accuracy: 0.8396 - val_loss: 0.7494 - val_accuracy: 0.5241\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.2827 - accuracy: 0.8846 - val_loss: 0.4201 - val_accuracy: 0.8068\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 124s 5ms/sample - loss: 0.2311 - accuracy: 0.9104 - val_loss: 0.3226 - val_accuracy: 0.8678\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.1828 - accuracy: 0.9269 - val_loss: 0.3097 - val_accuracy: 0.8790\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.1539 - accuracy: 0.9420 - val_loss: 0.4001 - val_accuracy: 0.8617\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.1299 - accuracy: 0.9511 - val_loss: 0.3560 - val_accuracy: 0.8771\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 121s 5ms/sample - loss: 0.1077 - accuracy: 0.9581 - val_loss: 0.4383 - val_accuracy: 0.8642\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 122s 5ms/sample - loss: 0.0889 - accuracy: 0.9675 - val_loss: 0.4117 - val_accuracy: 0.8700\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0760 - accuracy: 0.9721 - val_loss: 0.4521 - val_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8e89a3d250>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets add more CNN and BatchNorm layers to the network. Did this have the same affect as 2D CNNs from lab 5?\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We will use a sigmoid and a 1 neuron dense output since our data is binary.\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Nadam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do 1D CNNs and 2D CNNs behave the same from the changes we are making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at some LSTMs. LSTMs and RNNs in general were the racehorse of deep learning from 2014-2016. Now they have drastically fallen off of favor in the DL community. The questions we want to answer in this lab are: Why do you think this is? Do you think it was a mistake to stray away from RNNs? What changes do you think we could make to make them better or should we just drop them all together?\n",
    "\n",
    "The resources to learn more about this debate can be found here: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
    "\n",
    "and here: https://towardsdatascience.com/memory-attention-sequences-37456d271992\n",
    "\n",
    "and here: https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c\n",
    "\n",
    "These are optional readings but they serve to give you a firm foundation on the knowledge of current deep learning thought. Feel free to answer the above questions after we train our LSTMs.\n",
    "\n",
    "If you don't know anything about RNNs read this: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 645s 26ms/sample - loss: 0.5076 - accuracy: 0.7422 - val_loss: 0.3736 - val_accuracy: 0.8332\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 651s 26ms/sample - loss: 0.3895 - accuracy: 0.8286 - val_loss: 0.3517 - val_accuracy: 0.8426\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 673s 27ms/sample - loss: 0.3244 - accuracy: 0.8684 - val_loss: 0.3308 - val_accuracy: 0.8618\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 605s 24ms/sample - loss: 0.2752 - accuracy: 0.8954 - val_loss: 0.3608 - val_accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 638s 26ms/sample - loss: 0.2334 - accuracy: 0.9126 - val_loss: 0.3458 - val_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8e88a6ed10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will make our LSTMs. We will use a smaller batch size as they take longer to train.\n",
    "# We use the same embedding layers as we did for our CNNs.\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "\n",
    "# Here we will add in our LSTM layers. They should be directly after the embedding layer.\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Now we will cast the LSTM output to a dense layer to sort it. If you haven't noticed, thick dense layers at the end of networks are how every model 'collects its thoughts'.\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the basic LSTM compare to the 1D CNN? Is it overfitting as much? is it's testing accuracy better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 857s 34ms/sample - loss: 0.5148 - accuracy: 0.7429 - val_loss: 0.3459 - val_accuracy: 0.8498\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 851s 34ms/sample - loss: 0.3327 - accuracy: 0.8635 - val_loss: 0.3114 - val_accuracy: 0.8666\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 848s 34ms/sample - loss: 0.2757 - accuracy: 0.8922 - val_loss: 0.3084 - val_accuracy: 0.8760\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 851s 34ms/sample - loss: 0.2526 - accuracy: 0.9010 - val_loss: 0.3431 - val_accuracy: 0.8614\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 844s 34ms/sample - loss: 0.2215 - accuracy: 0.9158 - val_loss: 0.4130 - val_accuracy: 0.8472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efda0df4210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets add another LSTM layer to our model. Did that improve overfitting/accuracy?\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "\n",
    "# Here we will add in our LSTM layers. They should be directly after the embedding layer.\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "\n",
    "\n",
    "# Now we will cast the LSTM output to a dense layer to sort it. If you haven't noticed, thick dense layers at the end of networks are how every model 'collects its thoughts'.\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 2637s 105ms/sample - loss: 0.6202 - accuracy: 0.6646 - val_loss: 0.5071 - val_accuracy: 0.7749\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 2491s 100ms/sample - loss: 0.6119 - accuracy: 0.6644 - val_loss: 0.5388 - val_accuracy: 0.7497\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 2455s 98ms/sample - loss: 0.6125 - accuracy: 0.6332 - val_loss: 0.6907 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 2464s 99ms/sample - loss: 0.6860 - accuracy: 0.5069 - val_loss: 0.6802 - val_accuracy: 0.6789\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 2456s 98ms/sample - loss: 0.6814 - accuracy: 0.5236 - val_loss: 0.6973 - val_accuracy: 0.5070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd9b340450>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets use larger LSTM layers. What affect did that have? Why do you think that is based off of your knowledge of RNNs.\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "\n",
    "# Here we will add in our LSTM layers. They should be directly after the embedding layer.\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# Now we will cast the LSTM output to a dense layer to sort it. If you haven't noticed, thick dense layers at the end of networks are how every model 'collects its thoughts'.\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 1412s 56ms/sample - loss: 0.5333 - accuracy: 0.7414 - val_loss: 0.4053 - val_accuracy: 0.8234\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 1144s 46ms/sample - loss: 0.3469 - accuracy: 0.8562 - val_loss: 0.3402 - val_accuracy: 0.8553\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 1148s 46ms/sample - loss: 0.2741 - accuracy: 0.8908 - val_loss: 0.3580 - val_accuracy: 0.8442\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 1132s 45ms/sample - loss: 0.2344 - accuracy: 0.9088 - val_loss: 0.3206 - val_accuracy: 0.8736\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 1134s 45ms/sample - loss: 0.2008 - accuracy: 0.9218 - val_loss: 0.3255 - val_accuracy: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f03dac85b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets add Bi-directional layers to each of our RNNs. These make the model learn the data scanning both forwards and backwards. \n",
    "# Here is a detailed description: https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "\n",
    "# The bidirectional layer is a wrapper, you can apply it like so to each LSTM layer.\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000,50,input_length=400))\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('Nadam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=[x_test, y_test])\n",
    "\n",
    "\n",
    "#How does this affect training/overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you think of anyway to prevent overfitting in an LSTM? got down some ideas and feel free to try them. If you get a signifcant result post it to the discussion board for the rest of the class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at the classical examples of 1D CNNs and LSTMs, what do you think are the potential tradeoffs between using each one? Which one makes more sense to use and is there a reason to use LSTMs or RNNs in general for sequential data?\n",
    "\n",
    "If you are feeling brave and have the extra time I encourage you to impliment an attention layer for both the 1D CNN and bi-directional LSTM and see how much Attention helps. You can also use image attention layers to improve 2D CNNs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
